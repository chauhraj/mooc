{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the Amazon review data-set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] sframe.cython.cy_server: SFrame v2.1 started. Logging /tmp/sframe_server_1475129310.log\n"
     ]
    }
   ],
   "source": [
    "import sframe as sf\n",
    "products = sf.SFrame(\"../data/amazon_baby.gl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    import string\n",
    "    return text.translate(None, string.punctuation) \n",
    "\n",
    "products['review_clean'] = products['review'].apply(remove_punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "products = products[products['rating'] != 3] # ignore all the neutral ratings\n",
    "\n",
    "# Assign Positive and Negative ratings according to the reviews.\n",
    "products['sentiment'] =  products['rating'].apply(lambda rating : +1 if rating > 3 else -1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into training and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = products.random_split(.8, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Word Vector\n",
    "\n",
    "Now build the bag-of-words vector per review. Bag of words is essentially the count of words in each review i,e it is a count of each word that indicates how many times a word has appeared in that particular review."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "vectorizer = CountVectorizer(token_pattern=r'\\b\\w+\\b')\n",
    "     # Use this token pattern to keep single-letter words\n",
    "# First, learn vocabulary from the training data and assign columns to words\n",
    "# Then convert the training data into a sparse matrix\n",
    "train_matrix = vectorizer.fit_transform(train_data['review_clean'])\n",
    "# Second, convert the test data into a sparse matrix, using the same word-column mapping\n",
    "test_matrix = vectorizer.transform(test_data['review_clean'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression()\n",
    "model.fit(train_matrix, train_data['sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy: 0.932175425966\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_true=test_data['sentiment'].to_numpy(), y_pred=model.predict(test_matrix))\n",
    "print \"Test Accuracy: %s\" % accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline accuracy (majority class classifier): 0.842782577394\n"
     ]
    }
   ],
   "source": [
    "from __future__ import division\n",
    "baseline = len(test_data[test_data['sentiment'] == 1])/len(test_data)\n",
    "print \"Baseline accuracy (majority class classifier): %s\" % baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz question**: Using accuracy as the evaluation metric, was our logistic regression model better than the baseline (majority class classifier)? <span style=\"color:red\">*YES*</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " target_label | predicted_label | count \n",
      "--------------+-----------------+-------\n",
      "     -1       |       -1        |  3786\n",
      "     -1       |        1        |  1455\n",
      "      1       |       -1        |   806\n",
      "      1       |        1        | 27289\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cmat = confusion_matrix(y_true=test_data['sentiment'].to_numpy(),\n",
    "                        y_pred=model.predict(test_matrix),\n",
    "                        labels=model.classes_)    # use the same order of class as the LR model.\n",
    "print ' target_label | predicted_label | count '\n",
    "print '--------------+-----------------+-------'\n",
    "# Print out the confusion matrix.\n",
    "# NOTE: Your tool may arrange entries in a different order. Consult appropriate manuals.\n",
    "for i, target_label in enumerate(model.classes_):\n",
    "    for j, predicted_label in enumerate(model.classes_):\n",
    "        print '{0:^13} | {1:^15} | {2:5d}'.format(target_label, predicted_label, cmat[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fractional: 0.0506192596716 0.0517885744794 0.971311621285\n"
     ]
    }
   ],
   "source": [
    "totalPos = 1455 + 27289\n",
    "actualPos = 27289 + 806\n",
    "print \"fractional:\", (1455/totalPos), 1455/actualPos, 27289/actualPos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_cmat(y_true, y_pred, labels):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    cmat = confusion_matrix(y_true=test_data['sentiment'].to_numpy(),\n",
    "                            y_pred=y_pred,\n",
    "                            labels=labels)    # use the same order of class as the LR model.\n",
    "    print ' target_label | predicted_label | count '\n",
    "    print '--------------+-----------------+-------'\n",
    "    # Print out the confusion matrix.\n",
    "    # NOTE: Your tool may arrange entries in a different order. Consult appropriate manuals.\n",
    "    for i, target_label in enumerate(model.classes_):\n",
    "        for j, predicted_label in enumerate(model.classes_):\n",
    "            print '{0:^13} | {1:^15} | {2:5d}'.format(target_label, predicted_label, cmat[i,j])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "TN = 3786\n",
    "FP = 1455\n",
    "FN = 806\n",
    "TP = 27289\n",
    "TOTAL = TN + FP + FN + TP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: How many predicted values in the test set are false positives? <span style=\"color:blue\">**1455**</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quiz Question: Given the stipulation, what is the cost associated with the logistic regression classifier's performance on the test set?  146306\n"
     ]
    }
   ],
   "source": [
    "cost = FP*100 + FN*1\n",
    "print \"Quiz Question: Given the stipulation, what is the cost associated with the logistic regression classifier's performance on the test set? \", cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision on test data: 0.949380740328\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_score\n",
    "precision = precision_score(y_true=test_data['sentiment'].to_numpy(), \n",
    "                            y_pred=model.predict(test_matrix))\n",
    "print \"Precision on test data: %s\" % precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fraction of FP 0.0506192596716\n"
     ]
    }
   ],
   "source": [
    "print \"Fraction of FP\", FP/(FP + TP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recall on test data: 0.971311621285\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score\n",
    "recall = recall_score(y_true=test_data['sentiment'].to_numpy(),\n",
    "                      y_pred=model.predict(test_matrix))\n",
    "print \"Recall on test data: %s\" % recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "recall = TP/(TP+FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9713116212849261"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_threshold(probabilities, threshold):\n",
    "    def tApply(a, b):\n",
    "        if(a > b):\n",
    "            return +1\n",
    "        else:\n",
    "            return -1\n",
    "    vfunc = np.vectorize(tApply)\n",
    "    return vfunc(probabilities, threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "probabilities = model.predict_proba(test_matrix)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "t_05 = apply_threshold(probabilities, 0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_results(array):\n",
    "    print \"Positives:\", (array == +1).sum(), \" and Negatives:\", (array == -1).sum() \n",
    "\n",
    "t_09 = apply_threshold(probabilities, 0.9)    \n",
    "print \"Actual:\", print_results(test_data['sentiment'].to_numpy())\n",
    "print \"Predicted:\", print_results(t_05)\n",
    "print \"Predicted:\", print_results(t_09)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "recall is fraction of TP. As we increase the threshold, the TP decreases and FN increases which will bring down the recall value. Also, as we increase the threshold, predicted +ve decreases as compared to Actual +ves, i.e FP decreases. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "threshold_values = np.linspace(0.5, 1, num=100)\n",
    "print threshold_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_pr_curve(precision, recall, title):\n",
    "    plt.rcParams['figure.figsize'] = 7, 5\n",
    "    plt.locator_params(axis = 'x', nbins = 5)\n",
    "    plt.plot(precision, recall, 'b-', linewidth=4.0, color = '#B0017F')\n",
    "    plt.title(title)\n",
    "    plt.xlabel('Precision')\n",
    "    plt.ylabel('Recall')\n",
    "    plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = test_data['sentiment'].to_numpy()\n",
    "precision_all = [precision_score(y_true=y, y_pred=apply_threshold(probabilities, threshold)) for threshold in threshold_values]\n",
    "recall_all = [recall_score(y_true=y, y_pred=apply_threshold(probabilities, threshold)) for threshold in threshold_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_pr_curve(precision_all, recall_all, 'Precision recall curve (all)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = apply_threshold(probabilities, 0.98)\n",
    "print_cmat(y, y_pred, model.classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "precision_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx = np.argmin(np.abs(np.asarray(precision_all) - 0.965))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "idx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "threshold_values[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baby_reviews = test_data[test_data['name'].apply(lambda x: 'baby' in x.lower())]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "baby_matrix = vectorizer.transform(baby_reviews['review_clean'])\n",
    "probabilities = model.predict_proba(baby_matrix)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "precision_all = [precision_score(y_true=baby_reviews['sentiment'].to_numpy(), y_pred=apply_threshold(probabilities, threshold)) for threshold in threshold_values]\n",
    "recall_all = [recall_score(y_true=baby_reviews['sentiment'].to_numpy(), y_pred=apply_threshold(probabilities, threshold)) for threshold in threshold_values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
